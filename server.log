nohup: ignoring input
Dataset already exists. Skipping download.

journey to a patient (for I had now returned to civil practice), when
my way led me through Baker Street. As I passed the well-remembered
door, which must always be associated in my mind with my wooing, and
with the dark incidents of the Study in Sc
... excerpt from training data.
vocab_size: 98
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [03:33<00:00, 213.58s/it]100%|██████████| 1/1 [03:33<00:00, 213.58s/it]
Batch 0 loss = 4.5992
Batch 100 loss = 2.9450
Batch 200 loss = 2.6577
Batch 300 loss = 2.5169
Batch 400 loss = 2.4194
Batch 500 loss = 2.3028
Epoch 1 loss: 2.6310
W
a
t
s
o
n
 
w
a
s
 
s
i
t
t
i
n
g
 
b
y
 
t
h
e
 
a
r
m
c
h
a
i
r
,
 
a
n
d
 


m
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [03:34<07:09, 214.82s/it] 67%|██████▋   | 2/3 [07:07<03:33, 213.80s/it]100%|██████████| 3/3 [10:42<00:00, 214.00s/it]100%|██████████| 3/3 [10:42<00:00, 214.05s/it]
Batch 0 loss = 2.2045
Batch 100 loss = 2.0262
Batch 200 loss = 1.8637
Batch 300 loss = 1.7391
Batch 400 loss = 1.6155
Batch 500 loss = 1.5251
Epoch 1 loss: 1.7815
Batch 0 loss = 1.4795
Batch 100 loss = 1.4043
Batch 200 loss = 1.3415
Batch 300 loss = 1.2892
Batch 400 loss = 1.2583
Batch 500 loss = 1.2261
Epoch 2 loss: 1.3171
Batch 0 loss = 1.1915
Batch 100 loss = 1.1658
Batch 200 loss = 1.1270
Batch 300 loss = 1.0949
Batch 400 loss = 1.0712
Batch 500 loss = 1.0466
Epoch 3 loss: 1.1074
W
a
t
s
o
n
 
w
a
s
 
s
i
t
t
i
n
g
 
b
y
 
t
h
e
 
a
r
m
c
h
a
i
r
,
 
a
n
d
.
.
.
 
  0%|          | 0/3 [00:00<?, ?it/s]